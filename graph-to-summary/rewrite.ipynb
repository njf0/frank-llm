{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.02s/it]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"/nfs/public/hf/models/meta-llama/Meta-Llama-3-8B-Instruct\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"/nfs/public/hf/models/meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    # token='hf_xxx'\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    local_files_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.padding_side = 'left'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assemble_messages(data, system_content):\n",
    "\n",
    "\n",
    "    inputs = []\n",
    "    for example in data:\n",
    "        for explanation in example['explanation']:\n",
    "            user_content = \"\"\n",
    "            user_content += f\"Question: {example['question']} Explanation: {explanation}.\"\n",
    "\n",
    "            messages = [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": system_content\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": user_content\n",
    "                }\n",
    "            ]\n",
    "\n",
    "            inputs.append(messages)\n",
    "\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(inputs):\n",
    "\n",
    "    input_messages = tokenizer.apply_chat_template(inputs, padding=True, tokenize=True, return_tensors=\"pt\", add_generation_prompt=True).to(device)\n",
    "    outputs = model.generate(input_messages, max_new_tokens=256)\n",
    "    responses = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(path, system_content, data, responses):\n",
    "\n",
    "    for e, r in zip(data, responses):\n",
    "\n",
    "        # check if the key already exists\n",
    "        if 'llm-rewrites' not in e.keys():\n",
    "            e['llm-rewrites'] = []\n",
    "\n",
    "        result = {\n",
    "            'system_prompt': system_content,\n",
    "            'response': r\n",
    "        }\n",
    "        e['llm-rewrites'].append(result)\n",
    "\n",
    "    with path.open(mode='w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "system_content = \"'You are a query answering system explaining the methodology used to generate an answer. Rewrite the following explanation while using formal language and reducing redundancy, but do not add new information. Round population figures two three significant figures - for example, 69112701.18 rounds to 69.1 million, 65343184.0 rounds to 65.3 million.\"\n",
    "\n",
    "path = Path(\"../resources/rewriting/examples.json\")\n",
    "with path.open(encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "inputs = assemble_messages(data, system_content)\n",
    "responses = generate_response(inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results(path, system_content, data, responses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
