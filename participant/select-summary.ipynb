{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
                        "  from .autonotebook import tqdm as notebook_tqdm\n"
                    ]
                }
            ],
            "source": [
                "import json\n",
                "from pathlib import Path\n",
                "\n",
                "import pandas as pd\n",
                "import random\n",
                "import torch\n",
                "from transformers import AutoModelForCausalLM, AutoTokenizer"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
                        "Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.05it/s]\n"
                    ]
                }
            ],
            "source": [
                "tokenizer = AutoTokenizer.from_pretrained(\"/nfs/public/hf/models/meta-llama/Meta-Llama-3-8B-Instruct\")\n",
                "model = AutoModelForCausalLM.from_pretrained(\"/nfs/public/hf/models/meta-llama/Meta-Llama-3-8B-Instruct\",\n",
                "    # token='hf_xxx'\n",
                "    device_map=\"auto\",\n",
                "    trust_remote_code=True,\n",
                "    torch_dtype=torch.bfloat16,\n",
                "    local_files_only=True\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
                "model.to(device)\n",
                "\n",
                "tokenizer.pad_token = tokenizer.eos_token\n",
                "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
                "tokenizer.padding_side = 'left'"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [],
            "source": [
                "def load_data(\n",
                "        template: str | Path,\n",
                "    ) -> dict:\n",
                "\n",
                "    if isinstance(template, Path):\n",
                "        path = template\n",
                "\n",
                "    else:\n",
                "        path = Path('..', 'resources', 'data', f'{template}.json')\n",
                "\n",
                "    if not path.exists():\n",
                "        raise FileNotFoundError(f'The template {template} does not exist.')\n",
                "    with path.open(encoding='utf-8') as f:\n",
                "        data = json.load(f)\n",
                "\n",
                "    return data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [],
            "source": [
                "def assemble_messages(data, system_content):\n",
                "\n",
                "    inputs = []\n",
                "    for example in data.values():\n",
                "\n",
                "        user_content = \"\"\n",
                "        user_content += f\"Question: {example['question']}.\\n\\nProcess: \"\n",
                "        for step in example['explanation']:\n",
                "            user_content += f\"{step['step']}. {step['explanation']}. \"\n",
                "\n",
                "        messages = [\n",
                "            {\n",
                "                \"role\": \"system\",\n",
                "                \"content\": system_content\n",
                "            },\n",
                "            {\n",
                "                \"role\": \"user\",\n",
                "                \"content\": user_content\n",
                "            }\n",
                "        ]\n",
                "\n",
                "        inputs.append(messages)\n",
                "\n",
                "    return inputs"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [],
            "source": [
                "def generate_response(inputs):\n",
                "\n",
                "    input_messages = tokenizer.apply_chat_template(inputs, padding=True, tokenize=True, return_tensors=\"pt\", add_generation_prompt=True).to(device)\n",
                "    outputs = model.generate(input_messages, max_new_tokens=256)\n",
                "    responses = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
                "\n",
                "    return responses"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [],
            "source": [
                "def save_results(path, system_content, data, responses):\n",
                "\n",
                "    for k, r in zip(data, responses):\n",
                "\n",
                "        # check if the key already exists\n",
                "        if 'llm-selection' not in data[k]:\n",
                "            data[k]['llm-selection'] = []\n",
                "\n",
                "        result = {\n",
                "            'system_prompt': system_content,\n",
                "            'response': r\n",
                "        }\n",
                "        data[k]['llm-selection'].append(result)\n",
                "\n",
                "    with path.open(mode='w', encoding='utf-8') as f:\n",
                "        json.dump(data, f, indent=4)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
                        "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
                        "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
                        "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
                        "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
                        "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
                        "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
                    ]
                }
            ],
            "source": [
                "system_content = f\"Here is a question and the process that was used to solve it. Select the steps of that process that are most important for inclusion in a summary explanation of that process. Format the output as a list containing the numbers of the selected steps. Output only a list in this format and no other content.\"\n",
                "\n",
                "BASE_DIR = Path('..', 'resources', 'data')\n",
                "templates = BASE_DIR.iterdir()\n",
                "\n",
                "for template in templates:\n",
                "    data = load_data(template)\n",
                "    inputs = assemble_messages(data, system_content)\n",
                "    responses = generate_response(inputs)\n",
                "    save_results(template, system_content, data, responses)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".env",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
