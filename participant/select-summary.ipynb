{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "import datetime\n",
                "import json\n",
                "import logging\n",
                "import random\n",
                "from pathlib import Path\n",
                "\n",
                "import pandas as pd\n",
                "import torch\n",
                "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
                "\n",
                "logging.getLogger('transformers').setLevel(logging.ERROR)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "class LLMParticipant:\n",
                "\n",
                "    def __init__(\n",
                "            self,\n",
                "            config: dict\n",
                "        ) -> None:\n",
                "\n",
                "        self.config = config\n",
                "        self.MODEL = config['MODEL']\n",
                "        self.TEMPERATURE = config['TEMPERATURE']\n",
                "        self.BATCH_SIZE = config['BATCH_SIZE']\n",
                "        self.EXAMPLE = config['EXAMPLE']\n",
                "        self.FORMAT_VERSION = config['FORMAT_VERSION']\n",
                "        self.SYSTEM_CONTENT = config['SYSTEM_CONTENT']\n",
                "        self.INPUTS = config['INPUTS']\n",
                "\n",
                "        self.data = self.load_input_questions()\n",
                "\n",
                "        print(f\"Loading model {self.MODEL}... \", end=\"\")\n",
                "        self.tokenizer = AutoTokenizer.from_pretrained(self.MODEL)\n",
                "        self.model = AutoModelForCausalLM.from_pretrained(self.MODEL,\n",
                "            # token='hf_xxx'\n",
                "            device_map=\"auto\",\n",
                "            trust_remote_code=True,\n",
                "            torch_dtype=torch.bfloat16,\n",
                "            # local_files_only=True\n",
                "        )\n",
                "        print(\"done.\")\n",
                "\n",
                "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
                "        self.model.to(self.device)\n",
                "\n",
                "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
                "        self.tokenizer.pad_token_id = self.tokenizer.eos_token_id\n",
                "        self.tokenizer.padding_side = 'left'\n",
                "\n",
                "    def load_input_questions(\n",
                "            self,\n",
                "        ) -> dict:\n",
                "\n",
                "        with Path(self.INPUTS).open(encoding='utf-8') as f:\n",
                "            data = json.load(f)\n",
                "\n",
                "        return data\n",
                "\n",
                "    def assemble_messages(\n",
                "            self,\n",
                "            data: dict,\n",
                "        ) -> list[list[dict]]:\n",
                "\n",
                "        inputs = []\n",
                "        for example in data.values():\n",
                "\n",
                "            user_content = \"\"\n",
                "            user_content += f\"Question: {example['question']}\\n\\nProcess: \"\n",
                "            for step in example['explanation']:\n",
                "                user_content += f\"{step['step']}. {step['explanation']}. \"\n",
                "\n",
                "            model_family = self.MODEL.split('/')[0]\n",
                "            if model_family in [\"\"]:\n",
                "\n",
                "                messages = [\n",
                "                    {\n",
                "                        \"role\": \"system\",\n",
                "                        \"content\": self.SYSTEM_CONTENT\n",
                "                    },\n",
                "                    {\n",
                "                        \"role\": \"user\",\n",
                "                        \"content\": user_content\n",
                "                    }\n",
                "                ]\n",
                "\n",
                "            elif model_family in ['mistralai']:\n",
                "                messages = [\n",
                "                    {\n",
                "                        \"role\": \"user\",\n",
                "                        \"content\": self.SYSTEM_CONTENT + ' ' + user_content\n",
                "                    },\n",
                "                ]\n",
                "\n",
                "            inputs.append(messages)\n",
                "\n",
                "        return inputs\n",
                "\n",
                "    def apply_template_generate_response(\n",
                "            self,\n",
                "            inputs\n",
                "        ) -> list[str]:\n",
                "\n",
                "        responses = []\n",
                "        batched_inputs = [inputs[i:i+self.BATCH_SIZE] for i in range(0, len(inputs), self.BATCH_SIZE)]\n",
                "        for batch in batched_inputs:\n",
                "            input_messages = self.tokenizer.apply_chat_template(batch, padding=True, tokenize=True, return_tensors=\"pt\", add_generation_prompt=True).to(self.device)\n",
                "            outputs = self.model.generate(input_messages,\n",
                "                                          max_new_tokens=256,\n",
                "                                          do_sample=True,\n",
                "                                          temperature=self.TEMPERATURE\n",
                "                                          )\n",
                "            responses += self.tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
                "\n",
                "        return responses\n",
                "\n",
                "    def save_results(\n",
                "            self,\n",
                "            results: list[dict]\n",
                "        ):\n",
                "\n",
                "        output_filename = datetime.datetime.isoformat(datetime.datetime.now())\n",
                "        output_path = Path(\"outputs\", f\"{Path(self.INPUTS).stem}\", f\"{output_filename}.json\")\n",
                "        log = Path(\"..\", \"outputs\", f\"{Path(self.INPUTS).stem}\", 'log.json')\n",
                "        print(f\"Saving results in {output_path}... \", end=\"\")\n",
                "\n",
                "        outputs = {\n",
                "            \"config\": self.config,\n",
                "            \"results\": results\n",
                "        }\n",
                "\n",
                "        with output_path.open('w', encoding='utf-8') as f:\n",
                "            json.dump(outputs, f, indent=4)\n",
                "        print(\"done.\")\n",
                "\n",
                "        with log.open('r', encoding='utf-8') as f:\n",
                "            log_data = json.load(f)\n",
                "\n",
                "        log_data[output_filename] = self.config\n",
                "        # sort log\n",
                "        log_data = dict(sorted(log_data.items()))\n",
                "\n",
                "        with log.open('w', encoding='utf-8') as f:\n",
                "            json.dump(log_data, f, indent=4)\n",
                "\n",
                "    def participate(\n",
                "            self\n",
                "        ):\n",
                "\n",
                "        print(f\"Assembling messages for {len(self.data)} inputs... \", end=\"\")\n",
                "        inputs = self.assemble_messages(self.data)\n",
                "        print(\"done.\")\n",
                "        print(f\"Generating responses for {len(inputs)} inputs... \")\n",
                "        responses = self.apply_template_generate_response(inputs)\n",
                "        print(\"Done.\")\n",
                "        outputs = []\n",
                "        for q, r in zip(self.data.keys(), responses):\n",
                "            t = self.data[q]['template_id']\n",
                "            outputs.append({\n",
                "                \"question_id\": q,\n",
                "                \"template_id\": t,\n",
                "                \"response\": r,\n",
                "            })\n",
                "\n",
                "        self.save_results(outputs)\n",
                "\n",
                "        return outputs\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "MODEL = 'google/gemma-7b'\n",
                "TEMPERATURE = 0.3\n",
                "BATCH_SIZE = 20\n",
                "EXAMPLE = '[1, 2, 3, 4]'\n",
                "FORMAT_VERSION = 2\n",
                "SYSTEM_CONTENT = f\"Your role is to select, from a list the steps, those that are most important for inclusion in a summary explanation of that process. Format your output as a list, for example {EXAMPLE}. Output only this short summary paragraph and nothing else.\".format(EXAMPLE)\n",
                "INPUTS = '../resources/data/select.json'\n",
                "DESCRIPTION = \"testing\"\n",
                "\n",
                "config = {\n",
                "    \"MODEL\": MODEL,\n",
                "    \"TEMPERATURE\": TEMPERATURE,\n",
                "    \"BATCH_SIZE\": BATCH_SIZE,\n",
                "    \"EXAMPLE\": EXAMPLE,\n",
                "    \"FORMAT_VERSION\": FORMAT_VERSION,\n",
                "    \"SYSTEM_CONTENT\": SYSTEM_CONTENT,\n",
                "    \"INPUTS\": INPUTS,\n",
                "    \"DESCRIPTION\": DESCRIPTION\n",
                "}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loading model google/gemma-7b... "
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "74b05a402f5f4a729c3aa5fe870a94d2",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "50973448c33b4df19736548d569beb49",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "model.safetensors.index.json:   0%|          | 0.00/20.9k [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "97cc71b13bee443a858f3c56da8546c2",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "37b3a18c2a4f48379627791987234aff",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "model-00001-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "8a44ea256b094f93ab09c9c88405331f",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "model-00002-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "ff8311075e9649e89a66997c23aef113",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "model-00003-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "abab4ff8ca1e44f6a7922bb2b65bd466",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "model-00004-of-00004.safetensors:   0%|          | 0.00/2.11G [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "b6729e988e49423f916ce821ef8f0383",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "e724a37199944daaa99241cc89819585",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "done.\n",
                        "Assembling messages for 400 inputs... "
                    ]
                },
                {
                    "ename": "UnboundLocalError",
                    "evalue": "local variable 'messages' referenced before assignment",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
                        "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mLLMParticipant\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparticipate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
                        "Cell \u001b[0;32mIn[2], line 137\u001b[0m, in \u001b[0;36mLLMParticipant.participate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparticipate\u001b[39m(\n\u001b[1;32m    133\u001b[0m         \u001b[38;5;28mself\u001b[39m\n\u001b[1;32m    134\u001b[0m     ):\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAssembling messages for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m inputs... \u001b[39m\u001b[38;5;124m\"\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 137\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massemble_messages\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdone.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerating responses for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(inputs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m inputs... \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
                        "Cell \u001b[0;32mIn[2], line 81\u001b[0m, in \u001b[0;36mLLMParticipant.assemble_messages\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m model_family \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmistralai\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m     74\u001b[0m         messages \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     75\u001b[0m             {\n\u001b[1;32m     76\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     77\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mSYSTEM_CONTENT \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m user_content\n\u001b[1;32m     78\u001b[0m             },\n\u001b[1;32m     79\u001b[0m         ]\n\u001b[0;32m---> 81\u001b[0m     inputs\u001b[38;5;241m.\u001b[39mappend(\u001b[43mmessages\u001b[49m)\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m inputs\n",
                        "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'messages' referenced before assignment"
                    ]
                }
            ],
            "source": [
                "LLMParticipant(config=config).participate()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".env",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
