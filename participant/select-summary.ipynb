{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [],
            "source": [
                "import json\n",
                "from pathlib import Path\n",
                "\n",
                "import pandas as pd\n",
                "import random\n",
                "import torch\n",
                "from transformers import AutoModelForCausalLM, AutoTokenizer"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
                        "Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.15s/it]\n"
                    ]
                }
            ],
            "source": [
                "tokenizer = AutoTokenizer.from_pretrained(\"/nfs/public/hf/models/meta-llama/Meta-Llama-3-8B-Instruct\")\n",
                "model = AutoModelForCausalLM.from_pretrained(\"/nfs/public/hf/models/meta-llama/Meta-Llama-3-8B-Instruct\",\n",
                "    # token='hf_xxx'\n",
                "    device_map=\"auto\",\n",
                "    trust_remote_code=True,\n",
                "    torch_dtype=torch.bfloat16,\n",
                "    local_files_only=True\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [],
            "source": [
                "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
                "model.to(device)\n",
                "\n",
                "tokenizer.pad_token = tokenizer.eos_token\n",
                "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
                "tokenizer.padding_side = 'left'"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [],
            "source": [
                "def load_data(\n",
                "        template: str | Path,\n",
                "    ) -> dict:\n",
                "\n",
                "    if isinstance(template, Path):\n",
                "        path = template\n",
                "\n",
                "    else:\n",
                "        path = Path('..', 'resources', 'data', f'{template}.json')\n",
                "\n",
                "    if not path.exists():\n",
                "        raise FileNotFoundError(f'The template {template} does not exist.')\n",
                "    with path.open(encoding='utf-8') as f:\n",
                "        data = json.load(f)\n",
                "\n",
                "    return data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [],
            "source": [
                "def assemble_messages(data, system_content):\n",
                "\n",
                "    inputs = []\n",
                "    for example in data.values():\n",
                "\n",
                "        user_content = \"\"\n",
                "        user_content += f\"Question: {example['question']}\\n\\nProcess: \"\n",
                "        for step in example['explanation']:\n",
                "            user_content += f\"{step['step']}. {step['explanation']}. \"\n",
                "\n",
                "        messages = [\n",
                "            {\n",
                "                \"role\": \"system\",\n",
                "                \"content\": system_content\n",
                "            },\n",
                "            {\n",
                "                \"role\": \"user\",\n",
                "                \"content\": user_content\n",
                "            }\n",
                "        ]\n",
                "\n",
                "        inputs.append(messages)\n",
                "\n",
                "    return inputs"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {},
            "outputs": [],
            "source": [
                "def generate_response(inputs):\n",
                "\n",
                "    input_messages = tokenizer.apply_chat_template(inputs, padding=True, tokenize=True, return_tensors=\"pt\", add_generation_prompt=True).to(device)\n",
                "    outputs = model.generate(input_messages, max_new_tokens=256)\n",
                "    responses = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
                "\n",
                "    return responses"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "metadata": {},
            "outputs": [],
            "source": [
                "def save_results(path, system_content, data, responses):\n",
                "\n",
                "    for k, r in zip(data, responses):\n",
                "\n",
                "        # check if the key already exists\n",
                "        if 'llm-selection-summary' not in data[k]:\n",
                "            data[k]['llm-selection-summary'] = []\n",
                "\n",
                "        result = {\n",
                "            'system_prompt': system_content,\n",
                "            'response': r\n",
                "        }\n",
                "        data[k]['llm-selection-summary'].append(result)\n",
                "\n",
                "    with path.open(mode='w', encoding='utf-8') as f:\n",
                "        json.dump(data, f, indent=4)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
                        "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
                        "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
                        "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
                        "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
                        "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
                        "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
                        "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
                    ]
                }
            ],
            "source": [
                "system_content = f\"Here is a question and the process that was used to solve it. Select the steps of that process that are most important for inclusion in a summary explanation of that process. Then, rewrite these steps into a short summary. Output only this short summary paragraph and nothing else.\"\n",
                "\n",
                "BASE_DIR = Path('..', 'resources', 'data')\n",
                "templates = BASE_DIR.iterdir()\n",
                "\n",
                "for template in templates:\n",
                "    data = load_data(template)\n",
                "    inputs = assemble_messages(data, system_content)\n",
                "    responses = generate_response(inputs)\n",
                "    save_results(template, system_content, data, responses)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 27,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "system\n",
                        "\n",
                        "Here is a question and the process that was used to solve it. Select the steps of that process that are most important for inclusion in a summary explanation of that process. Then, rewrite these steps into a short summary. Output only this short summary paragraph and nothing else.user\n",
                        "\n",
                        "Question: In 2020, which country in Central Asia had the lowest energy consumption?\n",
                        "\n",
                        "Process: 1. A list of countries in Central Asia was needed. 2. 5 countries in Central Asia, including Uzbekistan, Turkmenistan and Kazakhstan, were found. 3. The energy consumption for each of these countries in 2020 was needed for a comparison. 4. Data for each country's energy consumption in 2020 was found. 5. The answer was found by comparing the values to each other.assistant\n",
                        "\n",
                        "Here is the most important steps of the process:\n",
                        "\n",
                        "1. A list of countries in Central Asia was needed.\n",
                        "2. The energy consumption for each of these countries in 2020 was found.\n",
                        "\n",
                        "Summary: In 2020, Turkmenistan had the lowest energy consumption in Central Asia.\n"
                    ]
                }
            ],
            "source": [
                "print(responses[8])"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".env",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
