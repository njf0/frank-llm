{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [
                {
                    "ename": "ModuleNotFoundError",
                    "evalue": "No module named 'config.LLMParticipantConfig'; 'config' is not a package",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
                        "Cell \u001b[0;32mIn[13], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpathlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mconfig\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mLLMParticipantConfig\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n",
                        "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'config.LLMParticipantConfig'; 'config' is not a package"
                    ]
                }
            ],
            "source": [
                "import datetime\n",
                "import json\n",
                "import random\n",
                "from pathlib import Path\n",
                "\n",
                "import pandas as pd\n",
                "import torch\n",
                "from transformers import AutoModelForCausalLM, AutoTokenizer"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "\n",
                "\n",
                "\n",
                "\n",
                "\n",
                "\n",
                "    def save_results(model, description, format_version):\n",
                "\n",
                "        description = input(\"Enter a description for this trial: \")\n",
                "\n",
                "        header = {\n",
                "        \"time\": datetime.datetime.isoformat(datetime.datetime.now()),\n",
                "        \"description\": description,\n",
                "        \"model\": MODEL,\n",
                "        \"example\": EXAMPLE,\n",
                "        \"format_version\": FORMAT_VERSION,\n",
                "        }"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [],
            "source": [
                "class LLMParticipant:\n",
                "\n",
                "    def __init__(\n",
                "            self,\n",
                "            config: dict\n",
                "        ) -> None:\n",
                "\n",
                "        self.config = config\n",
                "        self.MODEL = config.MODEL\n",
                "        self.TEMPERATURE = config.TEMPERATURE\n",
                "        self.EXAMPLE = config.EXAMPLE\n",
                "        self.FORMAT_VERSION = config.FORMAT_VERSION\n",
                "        self.SYSTEM_CONTENT = config.SYSTEM_CONTENT\n",
                "        self.INPUTS = config.INPUTS\n",
                "\n",
                "        self.data = self.load_input_questions()\n",
                "\n",
                "\n",
                "        # self.tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
                "        # self.model = AutoModelForCausalLM.from_pretrained(MODEL,\n",
                "        #     # token='hf_xxx'\n",
                "        #     device_map=\"auto\",\n",
                "        #     trust_remote_code=True,\n",
                "        #     torch_dtype=torch.bfloat16,\n",
                "        #     local_files_only=True\n",
                "        # )\n",
                "\n",
                "        # self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
                "        # self.model.to(device)\n",
                "\n",
                "        # self.tokenizer.pad_token = tokenizer.eos_token\n",
                "        # self.tokenizer.pad_token_id = tokenizer.eos_token_id\n",
                "        # self.tokenizer.padding_side = 'left'\n",
                "\n",
                "    def load_input_questions(\n",
                "            self,\n",
                "        ) -> dict:\n",
                "\n",
                "        with self.INPUTS.open(encoding='utf-8') as f:\n",
                "            data = json.load(f)\n",
                "\n",
                "        return data\n",
                "\n",
                "    def assemble_messages(\n",
                "            self,\n",
                "            data: dict,\n",
                "        ) -> list[list[dict]]:\n",
                "\n",
                "        inputs = []\n",
                "        for example in data.values():\n",
                "\n",
                "            user_content = \"\"\n",
                "            user_content += f\"Question: {example['question']}\\n\\nProcess: \"\n",
                "            for step in example['explanation']:\n",
                "                user_content += f\"{step['step']}. {step['explanation']}. \"\n",
                "\n",
                "            messages = [\n",
                "                {\n",
                "                    \"role\": \"system\",\n",
                "                    \"content\": self.SYSTEM_CONTENT\n",
                "                },\n",
                "                {\n",
                "                    \"role\": \"user\",\n",
                "                    \"content\": user_content\n",
                "                }\n",
                "            ]\n",
                "\n",
                "            inputs.append(messages)\n",
                "\n",
                "        return inputs\n",
                "\n",
                "    def apply_template_generate_response(\n",
                "            self,\n",
                "            inputs\n",
                "        ) -> list[str]:\n",
                "\n",
                "        input_messages = self.tokenizer.apply_chat_template(inputs, padding=True, tokenize=True, return_tensors=\"pt\", add_generation_prompt=True).to(device)\n",
                "        outputs = self.model.generate(input_messages, max_new_tokens=256, temperature=self.TEMPERATURE)\n",
                "        responses = [self.tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
                "\n",
                "        return responses\n",
                "\n",
                "    def save_results(\n",
                "            self,\n",
                "            results: list[dict]\n",
                "        ):\n",
                "\n",
                "        outputs = {\n",
                "            \"config\": self.config,\n",
                "            \"results\": results\n",
                "        }\n",
                "\n",
                "        output_filename = datetime.datetime.isoformat(datetime.datetime.now())\n",
                "\n",
                "        with Path(\"outputs\", f\"{self.INPUTS.stem}\", f\"{output_filename}.json\").open('w', encoding='utf-8') as f:\n",
                "            json.dump(outputs, f, indent=4)\n",
                "\n",
                "    def participate(\n",
                "            self\n",
                "        ):\n",
                "\n",
                "        inputs = self.assemble_messages(self.data)\n",
                "        responses = self.apply_template_generate_response(inputs)\n",
                "\n",
                "        outputs = []\n",
                "        for q, r in zip(self.data.keys(), responses):\n",
                "            t = self.data[q]['template_id']\n",
                "            outputs.append({\n",
                "                \"question_id\": q,\n",
                "                \"template_id\": t,\n",
                "                \"response\": r,\n",
                "            })\n",
                "\n",
                "        self.save_results(outputs)\n",
                "\n",
                "        return outputs\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [],
            "source": [
                "MODEL = 'nfs/public/hf/models/meta-llama/Meta-Llama-3-8B-Instruct'\n",
                "TEMPERATURE = 0.3\n",
                "EXAMPLE = '[1, 2, 3, 4]'\n",
                "FORMAT_VERSION = 2\n",
                "SYSTEM_CONTENT = f\"Your role is to select, from a list the steps, those that are most important for inclusion in a summary explanation of that process. Format your output as a list, for example {EXAMPLE}. Output only this short summary paragraph and nothing else.\"\n",
                "INPUTS = 'select.json'\n",
                "DESCRIPTION = input(\"Enter a description for this trial: \")\n",
                "\n",
                "config = {\n",
                "    \"MODEL\": MODEL,\n",
                "    \"TEMPERATURE\": TEMPERATURE,\n",
                "    \"EXAMPLE\": EXAMPLE,\n",
                "    \"FORMAT_VERSION\": FORMAT_VERSION,\n",
                "    \"SYSTEM_CONTENT\": SYSTEM_CONTENT,\n",
                "    \"INPUTS\": INPUTS,\n",
                "    \"DESCRIPTION\": DESCRIPTION\n",
                "}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [],
            "source": [
                "P = LLMParticipant(config=config)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".env",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.14"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
