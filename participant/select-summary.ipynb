{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "import datetime\n",
                "import json\n",
                "import logging\n",
                "import random\n",
                "from pathlib import Path\n",
                "\n",
                "import pandas as pd\n",
                "import torch\n",
                "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
                "\n",
                "logging.getLogger('transformers').setLevel(logging.ERROR)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "class LLMParticipant:\n",
                "\n",
                "    def __init__(\n",
                "            self,\n",
                "            config: dict\n",
                "        ) -> None:\n",
                "\n",
                "        self.config = config\n",
                "        self.MODEL = config['MODEL']\n",
                "        self.TEMPERATURE = config['TEMPERATURE']\n",
                "        self.BATCH_SIZE = config['BATCH_SIZE']\n",
                "        self.EXAMPLE = config['EXAMPLE']\n",
                "        self.FORMAT_VERSION = config['FORMAT_VERSION']\n",
                "        self.SYSTEM_CONTENT = config['SYSTEM_CONTENT']\n",
                "        self.INPUTS = config['INPUTS']\n",
                "\n",
                "        self.data = self.load_input_questions()\n",
                "\n",
                "        print(f\"Loading model {self.MODEL}... \", end=\"\")\n",
                "        self.tokenizer = AutoTokenizer.from_pretrained(self.MODEL)\n",
                "        self.model = AutoModelForCausalLM.from_pretrained(self.MODEL,\n",
                "            # token='hf_xxx'\n",
                "            device_map=\"auto\",\n",
                "            trust_remote_code=True,\n",
                "            torch_dtype=torch.bfloat16,\n",
                "            local_files_only=True\n",
                "        )\n",
                "        print(\"done.\")\n",
                "\n",
                "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
                "        self.model.to(self.device)\n",
                "\n",
                "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
                "        self.tokenizer.pad_token_id = self.tokenizer.eos_token_id\n",
                "        self.tokenizer.padding_side = 'left'\n",
                "\n",
                "    def load_input_questions(\n",
                "            self,\n",
                "        ) -> dict:\n",
                "\n",
                "        with Path(self.INPUTS).open(encoding='utf-8') as f:\n",
                "            data = json.load(f)\n",
                "\n",
                "        return data\n",
                "\n",
                "    def assemble_messages(\n",
                "            self,\n",
                "            data: dict,\n",
                "        ) -> list[list[dict]]:\n",
                "\n",
                "        inputs = []\n",
                "        for example in data.values():\n",
                "\n",
                "            user_content = \"\"\n",
                "            user_content += f\"Question: {example['question']}\\n\\nProcess: \"\n",
                "            for step in example['explanation']:\n",
                "                user_content += f\"{step['step']}. {step['explanation']}. \"\n",
                "\n",
                "            messages = [\n",
                "                {\n",
                "                    \"role\": \"system\",\n",
                "                    \"content\": self.SYSTEM_CONTENT\n",
                "                },\n",
                "                {\n",
                "                    \"role\": \"user\",\n",
                "                    \"content\": user_content\n",
                "                }\n",
                "            ]\n",
                "\n",
                "            inputs.append(messages)\n",
                "\n",
                "\n",
                "        return inputs\n",
                "\n",
                "    def apply_template_generate_response(\n",
                "            self,\n",
                "            inputs\n",
                "        ) -> list[str]:\n",
                "\n",
                "        responses = []\n",
                "        batched_inputs = [inputs[i:i+self.BATCH_SIZE] for i in range(0, len(inputs), self.BATCH_SIZE)]\n",
                "        for batch in batched_inputs:\n",
                "            input_messages = self.tokenizer.apply_chat_template(batch, padding=True, tokenize=True, return_tensors=\"pt\", add_generation_prompt=True).to(self.device)\n",
                "            outputs = self.model.generate(input_messages,\n",
                "                                          max_new_tokens=256,\n",
                "                                          do_sample=True,\n",
                "                                          temperature=self.TEMPERATURE\n",
                "                                          )\n",
                "            responses += self.tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
                "\n",
                "        return responses\n",
                "\n",
                "    def save_results(\n",
                "            self,\n",
                "            results: list[dict]\n",
                "        ):\n",
                "\n",
                "        output_filename = datetime.datetime.isoformat(datetime.datetime.now())\n",
                "        path = Path(\"outputs\", f\"{Path(self.INPUTS).stem}\", f\"{output_filename}.json\")\n",
                "        print(f\"Saving results in {path}... \", end=\"\")\n",
                "\n",
                "        outputs = {\n",
                "            \"config\": self.config,\n",
                "            \"results\": results\n",
                "        }\n",
                "\n",
                "        with path.open('w', encoding='utf-8') as f:\n",
                "            json.dump(outputs, f, indent=4)\n",
                "        print(\"done.\")\n",
                "\n",
                "    def participate(\n",
                "            self\n",
                "        ):\n",
                "\n",
                "        print(f\"Assembling messages for {len(self.data)} inputs... \", end=\"\")\n",
                "        inputs = self.assemble_messages(self.data)\n",
                "        print(\"done.\")\n",
                "        print(f\"Generating responses for {len(inputs)} inputs... \")\n",
                "        responses = self.apply_template_generate_response(inputs)\n",
                "        print(\"Done.\")\n",
                "        outputs = []\n",
                "        for q, r in zip(self.data.keys(), responses):\n",
                "            t = self.data[q]['template_id']\n",
                "            outputs.append({\n",
                "                \"question_id\": q,\n",
                "                \"template_id\": t,\n",
                "                \"response\": r,\n",
                "            })\n",
                "\n",
                "        self.save_results(outputs)\n",
                "\n",
                "        return outputs\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "MODEL = '/nfs/public/hf/models/meta-llama/Meta-Llama-3-8B-Instruct'\n",
                "TEMPERATURE = 0.3\n",
                "BATCH_SIZE = 20\n",
                "EXAMPLE = '[1, 2, 3, 4]'\n",
                "FORMAT_VERSION = 2\n",
                "SYSTEM_CONTENT = f\"Your role is to select, from a list the steps, those that are most important for inclusion in a summary explanation of that process. Format your output as a list, for example {EXAMPLE}. Output only this short summary paragraph and nothing else.\".format(EXAMPLE)\n",
                "INPUTS = '../resources/data/select.json'\n",
                "DESCRIPTION = \"testing\"\n",
                "\n",
                "config = {\n",
                "    \"MODEL\": MODEL,\n",
                "    \"TEMPERATURE\": TEMPERATURE,\n",
                "    \"BATCH_SIZE\": BATCH_SIZE,\n",
                "    \"EXAMPLE\": EXAMPLE,\n",
                "    \"FORMAT_VERSION\": FORMAT_VERSION,\n",
                "    \"SYSTEM_CONTENT\": SYSTEM_CONTENT,\n",
                "    \"INPUTS\": INPUTS,\n",
                "    \"DESCRIPTION\": DESCRIPTION\n",
                "}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loading model /nfs/public/hf/models/meta-llama/Meta-Llama-3-8B-Instruct... "
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "fd7cfa37e84449408a5730a0baccac9d",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "done.\n",
                        "Assembling messages for 400 inputs... done.\n",
                        "Generating responses for 400 inputs... \n"
                    ]
                }
            ],
            "source": [
                "LLMParticipant(config=config).participate()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".env",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
