{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import re\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# - agreement/consistency of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "STEP_NUM = {\n",
    "            'A1': 4,\n",
    "            'B1': 5,\n",
    "            'C1': 10,\n",
    "            'D1': 5,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnalyseLLMParticipant:\n",
    "    def __init__(self, output_file):\n",
    "        self.output_file = output_file\n",
    "        self.details, self.results = self.load_data()\n",
    "        self.questions = self.load_questions()\n",
    "        self.df = self.split_by_prompt()\n",
    "        self.parse_generation_for_selection()\n",
    "        self.response_eq_example()\n",
    "        self.response_length_gt_steps()\n",
    "        self.response_eq_all_steps()\n",
    "        self.meta_object_selection()\n",
    "\n",
    "    def load_data(self):\n",
    "        with open(self.output_file) as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        detail = data['config']\n",
    "        results = data['results']\n",
    "\n",
    "        return detail, results\n",
    "\n",
    "    def load_questions(self):\n",
    "\n",
    "        questions = Path(f'../resources/data/select.json')\n",
    "        with open(questions) as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        return data\n",
    "\n",
    "    def split_by_prompt(self):\n",
    "\n",
    "        qids = [r['question_id'] for r in self.results]\n",
    "        templates = [r['template_id'] for r in self.results]\n",
    "        responses = [r['response'] for r in self.results]\n",
    "\n",
    "        system_prompt = 'system\\n\\n'\n",
    "        user_prompt = \"user\\n\\n\"\n",
    "        generation_prompt = 'assistant\\n\\n'\n",
    "\n",
    "        system = []\n",
    "        user = []\n",
    "        generation = []\n",
    "\n",
    "        for response in responses:\n",
    "            system.append(response[len(system_prompt):response.index(user_prompt)])\n",
    "            user.append(response[response.index(user_prompt) + len(user_prompt):response.index(generation_prompt)])\n",
    "            generation.append(response[response.index(generation_prompt) + len(generation_prompt):])\n",
    "\n",
    "        # dictionary with {qid: [system, user, generation]}\n",
    "        return pd.DataFrame({'qid': qids, 'template': templates, 'num_steps': [STEP_NUM[t] for t in templates],\n",
    "                             'system': system, 'user': user, 'generation': generation})\n",
    "\n",
    "    def get_template_from_qid(\n",
    "            self,\n",
    "            qid,\n",
    "            return_steps_or_id: str = 'steps',\n",
    "        ):\n",
    "\n",
    "        template = self.questions[qid]['template_id']\n",
    "\n",
    "        if return_steps_or_id == 'steps':\n",
    "            return STEP_NUM[template]\n",
    "        else:\n",
    "            return template\n",
    "\n",
    "\n",
    "    def parse_generation_for_selection(self):\n",
    "\n",
    "        def matches_format(generation):\n",
    "            # regex for matching lists e.g., [1, 2, 3] or [a, b, c]\n",
    "            r = re.compile(r'\\[.*?\\]')\n",
    "            matches = r.findall(generation)\n",
    "            if matches:\n",
    "                match = matches[-1]\n",
    "                try:\n",
    "                    test = [str(m) for m in match[1:-1].split(',')]\n",
    "                except ValueError:\n",
    "                    test = None\n",
    "                return match\n",
    "\n",
    "        def valid_generation(template, generation):\n",
    "            if generation is not None:\n",
    "                try:\n",
    "                    valid = [int(m) for m in generation[1:-1].split(',')]\n",
    "                except ValueError:\n",
    "                    valid = None\n",
    "            else:\n",
    "                valid = None\n",
    "\n",
    "            # compare to number in different column\n",
    "            if valid is not None and len(valid) <= STEP_NUM[template]:\n",
    "                return valid\n",
    "\n",
    "            return None\n",
    "\n",
    "        self.df['matches_format'] = self.df['generation'].apply(lambda x: matches_format(x))\n",
    "        # compare to number in df['num_steps']\n",
    "        self.df['valid_generation'] = self.df[['template', 'matches_format']].apply(lambda x: valid_generation(x['template'], x['matches_format']), axis=1)\n",
    "\n",
    "\n",
    "    def response_eq_example(self):\n",
    "\n",
    "        example = self.details['EXAMPLE']\n",
    "        # count number of times example == df['matches_format']\n",
    "        self.df['response_eq_example'] = self.df['matches_format'].apply(lambda x: True if x == example else False)\n",
    "\n",
    "    def response_length_gt_steps(self):\n",
    "\n",
    "        # count cases where the assistant generation has more steps than the example\n",
    "        self.df['response_length_gt_steps'] = self.df.apply(lambda row: True if row['valid_generation'] is not None and len(row['valid_generation']) > row['num_steps'] else False, axis=1)\n",
    "\n",
    "    def response_eq_all_steps(self):\n",
    "\n",
    "        # count cases where assistant generation is just the full list of steps\n",
    "        self.df['response_eq_all_steps'] = self.df.apply(lambda row: True if row['valid_generation'] == [i for i in range(1, row['num_steps'] + 1)] else False, axis=1)\n",
    "\n",
    "    def meta_object_selection(self):\n",
    "\n",
    "        example = self.questions[list(self.questions.keys())[0]]\n",
    "        xp = example['explanation']\n",
    "        meta_steps = [i['step'] for i in xp if i['label'] == 'meta']\n",
    "        object_steps = [i['step'] for i in xp if i['label'] == 'object']\n",
    "\n",
    "        def meta_selections(generation):\n",
    "            if generation is not None:\n",
    "                return [i for i in generation if i in meta_steps]\n",
    "            return None\n",
    "\n",
    "        def meta_selections_pc(generation):\n",
    "            if generation is not None:\n",
    "                return len([i for i in generation if i in meta_steps]) / len(meta_steps)\n",
    "            return None\n",
    "\n",
    "        def object_selections(generation):\n",
    "            if generation is not None:\n",
    "                return [i for i in generation if i in object_steps]\n",
    "            return None\n",
    "\n",
    "        def object_selections_pc(generation):\n",
    "            if generation is not None:\n",
    "                return len([i for i in generation if i in object_steps]) / len(object_steps)\n",
    "            return None\n",
    "\n",
    "        self.df['meta_selections'] = self.df['valid_generation'].apply(lambda x: meta_selections(x))\n",
    "        self.df['meta_selections_pc'] = self.df['valid_generation'].apply(lambda x: meta_selections_pc(x))\n",
    "        self.df['object_selections'] = self.df['valid_generation'].apply(lambda x: object_selections(x))\n",
    "        self.df['object_selections_pc'] = self.df['valid_generation'].apply(lambda x: object_selections_pc(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL: /nfs/public/hf/models/meta-llama/Meta-Llama-3-8B-Instruct\n",
      "TEMPERATURE: 0.3\n",
      "BATCH_SIZE: 20\n",
      "EXAMPLE: [1, 2, 3, 4]\n",
      "FORMAT_VERSION: 2\n",
      "SYSTEM_CONTENT: Your role is to select, from a list the steps, those that are most important for inclusion in a summary explanation of that process. Format your output as a list, for example [1, 2, 3, 4]. Output only this short summary paragraph and nothing else.\n",
      "INPUTS: ../resources/data/select.json\n",
      "DESCRIPTION: testing\n"
     ]
    }
   ],
   "source": [
    "outputs = Path('outputs/select').iterdir()\n",
    "output = sorted(outputs)[-1]\n",
    "with output.open() as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "config = data['config']\n",
    "for k, v in config.items():\n",
    "    print(f'{k}: {v}')\n",
    "\n",
    "T = AnalyseLLMParticipant(output)\n",
    "T.df = T.df[T.df['template'] == 'A1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4]                    56\n",
       "[1, 2, 3]                       13\n",
       "[insert answer]                  2\n",
       "[4]                              1\n",
       "[1, 4]                           1\n",
       "[approximately 11.3 million]     1\n",
       "[1, 2, 4]                        1\n",
       "[1, 3, 4]                        1\n",
       "[1, 2, 9.8 million]              1\n",
       "Name: matches_format, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T.df['matches_format'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4]    56\n",
       "[1, 2, 3]       13\n",
       "[4]              1\n",
       "[1, 4]           1\n",
       "[1, 2, 4]        1\n",
       "[1, 3, 4]        1\n",
       "Name: valid_generation, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T.df['valid_generation'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     56\n",
       "False    44\n",
       "Name: response_eq_example, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T.df['response_eq_example'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    100\n",
       "Name: response_length_gt_steps, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T.df['response_length_gt_steps'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     56\n",
       "False    44\n",
       "Name: response_eq_all_steps, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T.df['response_eq_all_steps'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3]    70\n",
       "[1]        2\n",
       "[]         1\n",
       "Name: meta_selections, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T.df['meta_selections'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.000000     1\n",
       "0.333333     2\n",
       "0.666667    70\n",
       "Name: meta_selections_pc, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T.df['meta_selections_pc'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4]    57\n",
       "[2]       13\n",
       "[4]        3\n",
       "Name: object_selections, dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T.df['object_selections'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5    16\n",
       "1.0    57\n",
       "Name: object_selections_pc, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T.df['object_selections_pc'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# templates = ['A1', 'B1', 'C1', 'D1']\n",
    "\n",
    "# output = Path('outputs/2024-05-24T16:16:02.060434.json')\n",
    "# output = Path('outputs/2024-05-24T16:16:02.061578.json')\n",
    "# # output = Path('outputs/2024-05-24T16:16:02.062049.json')\n",
    "# with output.open() as f:\n",
    "#     data = json.load(f)\n",
    "\n",
    "\n",
    "# eq_example = []\n",
    "# length_gt_steps = []\n",
    "# eq_steps = []\n",
    "# for t in templates:\n",
    "#     T = PromptParticipant(output, t)\n",
    "#     # count occurences != none\n",
    "#     eq_example.append(sum(T.df['response_eq_example']))\n",
    "#     length_gt_steps.append(sum(T.df['response_length_gt_steps']))\n",
    "#     eq_steps.append(sum(T.df['response_eq_all_steps']))\n",
    "\n",
    "# # create dataframe from results\n",
    "# comp_df = pd.DataFrame({\n",
    "#     'template': templates,\n",
    "#     'num_steps': STEP_NUM.values(),\n",
    "#     'eq_example': eq_example,\n",
    "#     'length_gt_steps': length_gt_steps,\n",
    "#     'eq_steps': eq_steps\n",
    "# })\n",
    "\n",
    "# print(comp_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
